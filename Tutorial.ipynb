{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text preprocessing tutorial <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminaries (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"\"\"In the previous chapter, we saw examples of some common NLP\n",
    "applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our\n",
    "organization. We would normally walk through the requirements and break the\n",
    "problem down into several sub-problems, then try to develop a step-by-step\n",
    "procedure to solve them. Since language processing is involved, we would also\n",
    "list all the forms of text processing needed at each step. This step-by-step\n",
    "processing of text is known as pipeline. It is the series of steps involved in\n",
    "building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures\n",
    "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
    "in the workplace. Laying out and developing a text-processing pipeline is seen\n",
    "as a starting point for any NLP application development process. In this\n",
    "chapter, we will learn about the various steps involved and how they play\n",
    "important roles in solving the NLP problem and we’ll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we’ll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\"\"\n",
    "\n",
    "my_sent = sent_tokenize(mytext)\n",
    "my_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in my_sent:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Frequent Steps:\n",
    "    - Stop words remove\n",
    "    - Steaming and lemmatization\n",
    "    - removing digits / punctuation\n",
    "    - lowercasing,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords, punctuations, digits and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(texts):\n",
    "    my_stop_words = set(stopwords.words('english'))\n",
    "    # Convert texts to sentences\n",
    "    sents = sent_tokenize(texts)\n",
    "\n",
    "    def remove_stop_digit(word_tokens):\n",
    "        return [token.lower() for token in word_tokens if token not in my_stop_words\n",
    "                and token not in punctuation and not token.isdigit()] \n",
    "    \n",
    "    return [remove_stop_digit(word_tokenize(sent)) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'previous', 'chapter', 'saw', 'examples', 'common', 'nlp', 'applications', 'might', 'encounter', 'everyday', 'life'], ['if', 'asked', 'build', 'application', 'think', 'would', 'approach', 'organization'], ['we', 'would', 'normally', 'walk', 'requirements', 'break', 'problem', 'several', 'sub-problems', 'try', 'develop', 'step-by-step', 'procedure', 'solve'], ['since', 'language', 'processing', 'involved', 'would', 'also', 'list', 'forms', 'text', 'processing', 'needed', 'step'], ['this', 'step-by-step', 'processing', 'text', 'known', 'pipeline'], ['it', 'series', 'steps', 'involved', 'building', 'nlp', 'model'], ['these', 'steps', 'common', 'every', 'nlp', 'project', 'makes', 'sense', 'study', 'chapter'], ['understanding', 'common', 'procedures', 'nlp', 'pipeline', 'enable', 'us', 'get', 'started', 'nlp', 'problem', 'encountered', 'workplace'], ['laying', 'developing', 'text-processing', 'pipeline', 'seen', 'starting', 'point', 'nlp', 'application', 'development', 'process'], ['in', 'chapter', 'learn', 'various', 'steps', 'involved', 'play', 'important', 'roles', 'solving', 'nlp', 'problem', '’', 'see', 'guidelines', 'use', 'step'], ['in', 'later', 'chapters', '’', 'discuss', 'specific', 'pipelines', 'various', 'nlp', 'tasks', 'e.g.', 'chapters', '4–7']]\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_corpus(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>studying</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "      <td>studi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>quickly</td>\n",
       "      <td>quickli</td>\n",
       "      <td>quick</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>science</td>\n",
       "      <td>scienc</td>\n",
       "      <td>sci</td>\n",
       "      <td>scienc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>better</td>\n",
       "      <td>better</td>\n",
       "      <td>bet</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>study</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "      <td>studi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Original PorterStemmer LancasterStemmer SnowballStemmer\n",
       "63  studying         studi            study           studi\n",
       "49   quickly       quickli            quick           quick\n",
       "58   science        scienc              sci          scienc\n",
       "13    better        better              bet          better\n",
       "62     study         studi            study           studi"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\n",
    "    'adjusting', 'adjustment', 'adjustments', 'alumni', 'alumnus', 'am', 'are', 'ate', 'beautified', 'beautiful', 'beautifully', 'beautify', 'best', 'better', 'bigger', \n",
    "    'biggest', 'children', 'connected', 'connection', 'connections', 'connectivity', 'cried', 'cries', 'criteria', 'criterion', 'crying', 'democracy', 'democratization',\n",
    "    'democratize', 'discoveries', 'discovering','men', 'mice', 'misunderstanding', 'misunderstandings', 'more beautiful', 'most beautiful', \n",
    "    'multiplying', 'nationalities', 'nationality', 'nationalization', 'nationalize', 'nationalized', 'organization', 'organizations', 'phenomena', 'phenomenon', 'quicker', 'quickest', 'quickly', \n",
    "    'ran', 'relating', 'relational', 'relationally', 'remembering', 'runnable', 'runner', 'running','science', 'scientific', 'scientifically', 'studies', 'study', 'studying', 'troubled', \n",
    "    'troubles', 'troubling', 'understood', 'unknowingly', 'unknown', 'was', 'went', 'were', 'women', 'wondered', 'wondering', 'wonders', 'worse', 'worst'\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize stemmers\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Apply stemmers\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in words]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in words]\n",
    "snowball_stemmed = [snowball_stemmer.stem(word) for word in words]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Original': words,\n",
    "    'PorterStemmer': porter_stemmed,\n",
    "    'LancasterStemmer': lancaster_stemmed,\n",
    "    'SnowballStemmer': snowball_stemmed\n",
    "})\n",
    "\n",
    "# Print DataFrame\n",
    "df.sample(5)\n",
    "\n",
    "# ===================> SnowballStemmer is recommend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tuanphan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('better',pos='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better well\n"
     ]
    }
   ],
   "source": [
    "# Using spacy\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "token = sp(u'better')\n",
    "for word in token:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced pre-steps:\n",
    "- POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Charles PROPN Xxxxx True False\n",
      "Spencer Spencer PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "was be AUX xxx True True\n",
      "born bear VERB xxxx True False\n",
      "on on ADP xx True True\n",
      "16 16 NUM dd False False\n",
      "April April PROPN Xxxxx True False\n",
      "1889 1889 NUM dddd False False\n",
      "toHannah toHannah PROPN xxXxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "( ( PUNCT ( False False\n",
      "born bear VERB xxxx True False\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Harriet Harriet PROPN Xxxxx True False\n",
      "Pedlingham Pedlingham PROPN Xxxxx True False\n",
      "Hill Hill PROPN Xxxx True False\n",
      ") ) PUNCT ) False False\n",
      "and and CCONJ xxx True True\n",
      "Charles Charles PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "Sr Sr PROPN Xx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr')\n",
    "for token in doc:\n",
    "    print(token.text,token.lemma_,token.pos_,token.shape_,token.is_alpha,token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_in",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
